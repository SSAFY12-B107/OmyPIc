"""
OmyPIC STT/LLM ë©”íŠ¸ë¦­ ìƒì„± ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ Prometheus ë©”íŠ¸ë¦­ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.
Grafana ëŒ€ì‹œë³´ë“œ ë™ì‘ í™•ì¸ìš©ì…ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
  docker exec -it omypic-fastapi python /app/tests/generate_metrics.py

ë˜ëŠ” ë¡œì»¬ì—ì„œ:
  cd Backend
  python tests/generate_metrics.py
"""

import sys
import os
import time
import random
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def simulate_metrics():
    """ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ Prometheus ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜"""

    # Prometheus ë©”íŠ¸ë¦­ ì„í¬íŠ¸
    from core.metrics import (
        AUDIO_PROCESS_DURATION,
        LLM_API_DURATION,
        BACKGROUND_TASK_DURATION,
        ACTIVE_TASKS,
        PROBLEM_TYPE_EVALUATION_TIME,
        ERROR_COUNTER,
        normalize_label_value
    )

    logger.info("="*70)
    logger.info("OmyPIC ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
    logger.info("="*70)

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
    scenarios = [
        {"category": "past_experience", "stt_time": 3.5, "llm_time": 5.2},
        {"category": "description", "stt_time": 2.8, "llm_time": 4.8},
        {"category": "roleplay", "stt_time": 4.2, "llm_time": 6.1},
        {"category": "self_introduction", "stt_time": 2.5, "llm_time": 4.0},
        {"category": "comparison", "stt_time": 3.8, "llm_time": 5.5},
        {"category": "routine", "stt_time": 3.0, "llm_time": 4.5},
        {"category": "technology", "stt_time": 3.3, "llm_time": 5.8},
        {"category": "watching_movies", "stt_time": 2.9, "llm_time": 4.3},
    ]

    total_tests = 20  # ì´ í…ŒìŠ¤íŠ¸ ìˆ˜

    for i in range(total_tests):
        scenario = random.choice(scenarios)
        category = scenario["category"]

        # ëœë¤ ë³€ë™ ì¶”ê°€ (Â±30%)
        stt_time = scenario["stt_time"] * random.uniform(0.7, 1.3)
        llm_time = scenario["llm_time"] * random.uniform(0.7, 1.3)
        task_time = stt_time + llm_time + random.uniform(0.5, 2.0)

        logger.info(f"\n[{i+1}/{total_tests}] ì‹œë®¬ë ˆì´ì…˜ - ì¹´í…Œê³ ë¦¬: {category}")

        # 1. STT ë©”íŠ¸ë¦­ ê¸°ë¡
        AUDIO_PROCESS_DURATION.labels(
            status="success",
            processor="wit_ai"
        ).observe(stt_time)
        logger.info(f"  ğŸ¤ STT ì²˜ë¦¬ ì‹œê°„: {stt_time:.2f}ì´ˆ")

        # 2. LLM ë©”íŠ¸ë¦­ ê¸°ë¡
        LLM_API_DURATION.labels(
            provider="google",
            model="gemini_1_5_pro",
            operation="evaluate_response",
            status="success"
        ).observe(llm_time)
        logger.info(f"  ğŸ¤– LLM ì‘ë‹µ ì‹œê°„: {llm_time:.2f}ì´ˆ")

        # 3. Background Task ë©”íŠ¸ë¦­ ê¸°ë¡
        BACKGROUND_TASK_DURATION.labels(
            task_type="audio_evaluation",
            status="success"
        ).observe(task_time)
        logger.info(f"  âš™ï¸ ì‘ì—… ì²˜ë¦¬ ì‹œê°„: {task_time:.2f}ì´ˆ")

        # 4. ë¬¸ì œ ìœ í˜•ë³„ í‰ê°€ ì‹œê°„ ê¸°ë¡
        PROBLEM_TYPE_EVALUATION_TIME.labels(
            problem_category=category,
            status="success"
        ).observe(llm_time)
        logger.info(f"  ğŸ“ ë¬¸ì œ ìœ í˜•: {category}")

        # 5. í™œì„± ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ (ì¦ê°€ í›„ ê°ì†Œ)
        ACTIVE_TASKS.labels(task_type="audio_evaluation").inc()
        time.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
        ACTIVE_TASKS.labels(task_type="audio_evaluation").dec()

        # 6. ê°„í—ì  ì—ëŸ¬ ì‹œë®¬ë ˆì´ì…˜ (10% í™•ë¥ )
        if random.random() < 0.1:
            error_type = random.choice(["TimeoutError", "APIError", "ValidationError"])
            ERROR_COUNTER.labels(
                module="audio_processor" if random.random() < 0.5 else "evaluator",
                error_type=error_type
            ).inc()
            logger.info(f"  âš ï¸ ì—ëŸ¬ ì‹œë®¬ë ˆì´ì…˜: {error_type}")

        # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ê¹Œì§€ ì§§ì€ ëŒ€ê¸°
        time.sleep(0.3)

    # ì¶”ê°€: ë‹¤ì–‘í•œ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬ë¥¼ ìœ„í•œ ì¶”ê°€ ë©”íŠ¸ë¦­
    logger.info("\n" + "="*70)
    logger.info("ì¶”ê°€ ë©”íŠ¸ë¦­ ìƒì„± ì¤‘...")

    # ë¹ ë¥¸ ìš”ì²­ë“¤
    for _ in range(10):
        AUDIO_PROCESS_DURATION.labels(status="success", processor="wit_ai").observe(random.uniform(1.0, 2.5))
        LLM_API_DURATION.labels(provider="google", model="gemini_1_5_pro", operation="evaluate_response", status="success").observe(random.uniform(2.0, 4.0))

    # ëŠë¦° ìš”ì²­ë“¤
    for _ in range(5):
        AUDIO_PROCESS_DURATION.labels(status="success", processor="wit_ai").observe(random.uniform(5.0, 10.0))
        LLM_API_DURATION.labels(provider="google", model="gemini_1_5_pro", operation="evaluate_response", status="success").observe(random.uniform(8.0, 15.0))

    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "="*70)
    logger.info("ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ!")
    logger.info("="*70)
    logger.info(f"ì´ {total_tests + 15}ê°œì˜ ë©”íŠ¸ë¦­ í¬ì¸íŠ¸ ìƒì„±ë¨")
    logger.info("\nGrafanaì—ì„œ í™•ì¸:")
    logger.info("  - URL: http://localhost:3001")
    logger.info("  - ê³„ì •: admin / omypic123")
    logger.info("  - ëŒ€ì‹œë³´ë“œ: OmyPIC Backend Monitoring")
    logger.info("="*70)


if __name__ == "__main__":
    simulate_metrics()
